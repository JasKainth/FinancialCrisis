# -*- coding: utf-8 -*-
"""Finance.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UUSfCZJQm1T-ZaWh-fBji-1iNn21Y8G6

# **Financial Stocks Data**

For this dataset, we will perform an EDA on some stocks of banks from the financial crisis to early 2016. Later, we will also try to fit a regression model to try and capture the trend that the data follows.  
We will get the data for some of the biggest American Banks: 
* Bank of America
* CitiGroup
* Goldman Sachs
* JPMorgan Chase
* Morgan Stanley 
* Wells Fargo
"""

# Import some libraries
from pandas_datareader import data, wb # To get our data
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import datetime # For date format
import statsmodels.api as sm # For decomposing plots
sns.set_style("whitegrid") # Set a default seaborn style

# We will have the same start and end for all the banks
start = datetime.datetime(2006, 1, 1)
end = datetime.datetime(2020, 1, 1)

# Bank of America
BAC = data.DataReader("BAC", "yahoo", start, end)[["High", "Low", 
                                                   "Open", "Close", "Volume"]]
# For this we got the Date as an Index, High, Low, Open, Close, Volume and 
# Adjusted Close
# Grab everything other than the adjusted close
# CitiGroup
C = data.DataReader("C", "yahoo", start, end)[["High", "Low", 
                                                   "Open", "Close", "Volume"]]
# Goldman Sachs
GS = data.DataReader("GS", "yahoo", start, end)[["High", "Low", 
                                                   "Open", "Close", "Volume"]]
# JPMorgan Chase
JPM = data.DataReader("JPM", "yahoo", start, end)[["High", "Low", 
                                                   "Open", "Close", "Volume"]]
# Morgan Stanley
MS = data.DataReader("MS", "yahoo", start, end)[["High", "Low", 
                                                   "Open", "Close", "Volume"]]
# Wells Fargo
WFC = data.DataReader("WFC", "yahoo", start, end)[["High", "Low", 
                                                   "Open", "Close", "Volume"]]

# Create a list of tickers
tickers = "BAC C GS JPM MS WFC".split()
# Concatenate over columns (set axis = 1)
bank_stocks = pd.concat([BAC, C, GS, JPM, MS, WFC], axis = 1, keys = tickers)
# Add column names
bank_stocks.columns.names = ["Bank Ticker", "Stock Info"]
# What does our data look like right now?
bank_stocks.head()

"""# **EDA**

We will calculate the percent return for each stock. The return is defined as  
$r_t = \frac{p_t - p_{t-1}}{p_{t-1}} = \frac{p_t}{p_{t-1}} - 1$
"""

returns = pd.DataFrame()
for ticker in tickers:
  returns[ticker + " Return"] = bank_stocks[ticker]["Close"].pct_change()

# What does our returns df look like?
returns.head(10)
# Makes sense that the first one is NaN because we can't compare it to anything

# What trends do we see in the returns df?
sns.pairplot(returns[1:])
plt.show()
# Nothing seems too out of the ordinary, all of them are heavily centered 
# around 0

# What does the distribution of the returns look like over the years?
# Make another dataset in a different format;
# We will have 4 columns (3 and we create one later called Year)
# Make one column for the date, one for the bank and one for the return of the
# bank at that date
# Reset the index (easier to make plots with one index rather than multileve
# index)
long_banks = pd.DataFrame(returns.stack()).reset_index()
long_banks.columns = ["Date", "Bank", "Return"]
# Use a lammbda function to get the bank name (the entries right now are 
# "Bank_Name Return" so remove the "Return")
long_banks["Bank"] = long_banks["Bank"].apply(lambda x: x.split()[0])
# Get the year (for facet plot)
long_banks["Year"] = pd.DatetimeIndex(long_banks["Date"]).year
# Create a plot of the returns for each bank over the 14 years of data we have
g = sns.FacetGrid(long_banks, col = "Bank",  row = "Year", hue = "Bank")
g.map(sns.histplot, "Return")
plt.show()
# We see that in 2006 & 2007, there is very low variability
# However, in 2008 & 2009, there is a lot of variability and the peaks are not 
# as concentrated as the other years 
# After that, 2010 & 2011 is better than '08 & '09 but in 2012 & 2013 do we 
# finally see the returns get back to "normal"

# Lets compare data from 2 years (2008 and 2019) for each bank, side by side
long_banks_filtered = long_banks[(long_banks["Year"] == 2008) |
                                 (long_banks["Year"] == 2019)]
g = sns.FacetGrid(long_banks_filtered, col = "Year",  
                  row = "Bank", hue = "Bank")
g.map(sns.histplot, "Return")
plt.show()

# When do the highest and lowest returns occur for each bank?
returns.idxmin()
# WOW, so they are all heavily around the financial crisis (which makes sense)
# But one thing which was unexpected is that for FOUR out of the SIX banks, the
# Minimum occurs on the same day (BAC, GS, JPM & WFC) on 2009-01-20
# On January 20, 2009 is when Barack Obama for inaugurated.

# What about the max?
returns.idxmax()
# Funny thing about MS, 4 days after their lowest return they had their highest
# and for JPM the highest was the very next day

# During the financial crisis, there was a new president so there was a lot of
# changes occuring, which is why there is probably a lot of variability during
# that period. We will probably see this for all the banks when we plot the 
# trend lines for each of the banks (let's see how this changes the 7 day and
# 30 day rolling average, which we will also plot)

# What does the overall SD of each stock look like 
# Look at SD for the whole dataset, and also for 2007 - 2010
returns.std()
# BAC, C & MS have a relatively high std compared to GS, JPM & WFC

# Let's take a look at the std over 4 years
a = returns.loc["2007-01-01":"2007-12-31"].std()
b = returns.loc["2008-01-01":"2008-12-31"].std()
c = returns.loc["2009-01-01":"2009-12-31"].std()
d = returns.loc["2010-01-01":"2010-12-31"].std()

sd = pd.concat([a, b, c, d], axis = 1, keys = [2007, 2008, 2009, 2010])
# As we can see, the std is pretty low in 2007, but increases a lot in 
# 2008 & 2009 (which we expect) and begins to settle down again in 2010
sd

# What does the std look like in the most recent full year (2019)
returns.loc["2019-01-01":"2019-12-31"].std()
# They are all very low

# Let's take a look at the closing price for each stock
# How does it vary over time
# CitiGroup crashed quite a bit in 2008 and didn't really recover

# Resize the figure (and change the dpi to make it bigger/nicer)
plt.figure(figsize = (12, 4), dpi = 300)
# Run a for loop that will go over each ticker symbol (bank) and plot the Close
# price for each bank
for ticker in tickers:
  # Plot the data and add a label for the ticker
  bank_stocks[ticker]["Close"].plot(label = ticker)
plt.legend()
plt.xlabel("Date")
plt.ylabel("Closing Price")
plt.title("Closing Price of Bank Stocks")
plt.show()

# Let's see a breakdown of each Bank's trend
# BAC
# We will make the period a week (this will help us get rid of some NAs) but 
# will also make the format of the DataFrame a bit nicer
bac = bank_stocks["BAC"]["Close"].resample('W').mean()
# Decompose the trend (into an additive model)
decomposition = sm.tsa.seasonal_decompose(bac, model = "additive")
# Create the figure and axes (using the object oriented method)
# Note: We could have simply called the method .plot() on our decomposition 
# dataframe but that would not allow us to make any changes 
# So we would not be able to to change the figure size, dpi or color of the 
# plots
fig, axes = plt.subplots(4, 1, sharex=True, figsize = (8, 4), dpi = 300)
decomposition.observed.plot(ax=axes[0], legend=False)
axes[0].set_ylabel("Observed")
axes[0].set_title("Bank of America Close Price Decomposition")
decomposition.trend.plot(ax=axes[1], legend=False)
axes[1].set_ylabel("Trend")
decomposition.seasonal.plot(ax=axes[2], legend=False)
axes[2].set_ylabel("Seasonal")
decomposition.resid.plot(ax=axes[3], legend=False)
axes[3].set_ylabel("Residual")
plt.show()

# C
c = bank_stocks["C"]["Close"].resample('W').mean()
decomposition = sm.tsa.seasonal_decompose(c, model = "additive")
fig, axes = plt.subplots(4, 1, sharex=True, figsize = (8, 4), dpi = 300)
decomposition.observed.plot(ax=axes[0], legend=False, color = "orange")
axes[0].set_ylabel("Observed")
axes[0].set_title("CitiGroup Close Price Decomposition")
decomposition.trend.plot(ax=axes[1], legend=False, color = "orange")
axes[1].set_ylabel("Trend")
decomposition.seasonal.plot(ax=axes[2], legend=False, color = "orange")
axes[2].set_ylabel("Seasonal")
decomposition.resid.plot(ax=axes[3], legend=False, color = "orange")
axes[3].set_ylabel("Residual")
plt.show()

# GS
gs = bank_stocks["GS"]["Close"].resample('W').mean()
decomposition = sm.tsa.seasonal_decompose(gs, model = "additive")
fig, axes = plt.subplots(4, 1, sharex=True, figsize = (8, 4), dpi = 300)
decomposition.observed.plot(ax=axes[0], legend=False, color = "green")
axes[0].set_ylabel("Observed")
axes[0].set_title("Goldman Sachs Close Price Decomposition")
decomposition.trend.plot(ax=axes[1], legend=False, color = "green")
axes[1].set_ylabel("Trend")
decomposition.seasonal.plot(ax=axes[2], legend=False, color = "green")
axes[2].set_ylabel("Seasonal")
decomposition.resid.plot(ax=axes[3], legend=False, color = "green")
axes[3].set_ylabel("Residual")
plt.show()

# JPM
jpm = bank_stocks["JPM"]["Close"].resample('W').mean()
decomposition = sm.tsa.seasonal_decompose(jpm, model = "additive")
fig, axes = plt.subplots(4, 1, sharex=True, figsize = (8, 4), dpi = 300)
decomposition.observed.plot(ax=axes[0], legend=False, color = "red")
axes[0].set_ylabel("Observed")
axes[0].set_title("JPMorgan Close Price Decomposition")
decomposition.trend.plot(ax=axes[1], legend=False, color = "red")
axes[1].set_ylabel("Trend")
decomposition.seasonal.plot(ax=axes[2], legend=False, color = "red")
axes[2].set_ylabel("Seasonal")
decomposition.resid.plot(ax=axes[3], legend=False, color = "red")
axes[3].set_ylabel("Residual")
plt.show()

# MS
ms = bank_stocks["MS"]["Close"].resample('W').mean()
decomposition = sm.tsa.seasonal_decompose(ms, model = "additive")
fig, axes = plt.subplots(4, 1, sharex=True, figsize = (8, 4), dpi = 300)
decomposition.observed.plot(ax=axes[0], legend=False, color = "purple")
axes[0].set_ylabel("Observed")
axes[0].set_title("Morgan Stanley Close Price Decomposition")
decomposition.trend.plot(ax=axes[1], legend=False, color = "purple")
axes[1].set_ylabel("Trend")
decomposition.seasonal.plot(ax=axes[2], legend=False, color = "purple")
axes[2].set_ylabel("Seasonal")
decomposition.resid.plot(ax=axes[3], legend=False, color = "purple")
axes[3].set_ylabel("Residual")
plt.show()

# WFC
wfc = bank_stocks["WFC"]["Close"].resample('W').mean()
decomposition = sm.tsa.seasonal_decompose(wfc, model = "additive")
fig, axes = plt.subplots(4, 1, sharex=True, figsize = (8, 4), dpi = 300)
decomposition.observed.plot(ax=axes[0], legend=False, color = "brown")
axes[0].set_ylabel("Observed")
axes[0].set_title("Wells Fargo Close Price Decomposition")
decomposition.trend.plot(ax=axes[1], legend=False, color = "brown")
axes[1].set_ylabel("Trend")
decomposition.seasonal.plot(ax=axes[2], legend=False, color = "brown")
axes[2].set_ylabel("Seasonal")
decomposition.resid.plot(ax=axes[3], legend=False, color = "brown")
axes[3].set_ylabel("Residual")
plt.show()

"""**What do we see from those plots?**  
The first time that is noticeable from the trend is all of them take a dip in 2008-2009. Some of them, like WFC, recover but some of them, like C, don't. We also see that some banks have multiple dips. Most of them aren't too bad but in the case of GS, it is quite a large dip. It should be noted that the seasonal trend is negligible since when we take a look at the y-axes, the values are very small will often get overshadowed by the Residual.
"""

# For most of these banks, there was a lot of variability in 2008 & 2009
# So, let's take a look at the 30 day and 7 day rolling average during that 
# time period
# Will also add 2010 just so we see how the begin to recover
# BAC 
# Create the figure and specify the size and dpi
plt.figure(figsize = (12, 4), dpi = 300)
# Plot the Closing prices
bank_stocks["BAC"]["Close"].loc["2008-01-01":"2011-01-01"].\
  plot(label = "BAC Close Price")
# We use the rolling() function to create a 7 & 30 day rolling average
bank_stocks["BAC"]["Close"].loc["2008-01-01":"2011-01-01"].\
  rolling(window = 7).mean().plot(label = "7 Day Rolling Average")
bank_stocks["BAC"]["Close"].loc["2008-01-01":"2011-01-01"].\
  rolling(window = 30).mean().plot(label = "30 Day Rolling Average")
plt.legend()
plt.xlabel("Date")
plt.ylabel("Closing Price")
plt.title("Bank of America Closing Price")
plt.show()

# C
plt.figure(figsize = (12, 4), dpi = 300)
bank_stocks["C"]["Close"].loc["2008-01-01":"2011-01-01"].\
  plot(label = "C Close Price")
bank_stocks["C"]["Close"].loc["2008-01-01":"2011-01-01"].\
  rolling(window = 7).mean().plot(label = "7 Day Rolling Average")
bank_stocks["C"]["Close"].loc["2008-01-01":"2011-01-01"].\
  rolling(window = 30).mean().plot(label = "30 Day Rolling Average")
plt.legend()
plt.xlabel("Date")
plt.ylabel("Closing Price")
plt.title("CitiGroup Closing Price")
plt.show()

# GS
plt.figure(figsize = (12, 4), dpi = 300)
bank_stocks["GS"]["Close"].loc["2008-01-01":"2011-01-01"].\
  plot(label = "GS Close Price")
bank_stocks["GS"]["Close"].loc["2008-01-01":"2011-01-01"].\
  rolling(window = 7).mean().plot(label = "7 Day Rolling Average")
bank_stocks["GS"]["Close"].loc["2008-01-01":"2011-01-01"].\
  rolling(window = 30).mean().plot(label = "30 Day Rolling Average")
plt.legend()
plt.xlabel("Date")
plt.ylabel("Closing Price")
plt.title("Goldman Sachs Closing Price")
plt.show()

# JPM
plt.figure(figsize = (12, 4), dpi = 300)
bank_stocks["JPM"]["Close"].loc["2008-01-01":"2011-01-01"].\
  plot(label = "JPM Close Price")
bank_stocks["JPM"]["Close"].loc["2008-01-01":"2011-01-01"].\
  rolling(window = 7).mean().plot(label = "7 Day Rolling Average")
bank_stocks["JPM"]["Close"].loc["2008-01-01":"2011-01-01"].\
  rolling(window = 30).mean().plot(label = "30 Day Rolling Average")
plt.legend()
plt.xlabel("Date")
plt.ylabel("Closing Price")
plt.title("JPMorgan Closing Price")
plt.show()

# MS
plt.figure(figsize = (12, 4), dpi = 300)
bank_stocks["MS"]["Close"].loc["2008-01-01":"2011-01-01"].\
  plot(label = "MS Close Price")
bank_stocks["MS"]["Close"].loc["2008-01-01":"2011-01-01"].\
  rolling(window = 7).mean().plot(label = "7 Day Rolling Average")
bank_stocks["MS"]["Close"].loc["2008-01-01":"2011-01-01"].\
  rolling(window = 30).mean().plot(label = "30 Day Rolling Average")
plt.legend()
plt.xlabel("Date")
plt.ylabel("Closing Price")
plt.title("Morgan Stanley Closing Price")
plt.show()

# WFC
plt.figure(figsize = (12, 4), dpi = 300)
bank_stocks["WFC"]["Close"].loc["2008-01-01":"2011-01-01"].\
  plot(label = "WFC Close Price")
bank_stocks["WFC"]["Close"].loc["2008-01-01":"2011-01-01"].\
  rolling(window = 7).mean().plot(label = "7 Day Rolling Average")
bank_stocks["WFC"]["Close"].loc["2008-01-01":"2011-01-01"].\
  rolling(window = 30).mean().plot(label = "30 Day Rolling Average")
plt.legend()
plt.xlabel("Date")
plt.ylabel("Closing Price")
plt.title("Wells Fargo Closing Price")
plt.show()

# How are the stock prices correlated among each bank? (Look at closing price)
# Create a correlation matrix
corr_matrix = bank_stocks.xs(key = "Close", axis = 1, level = "Stock Info").\
              corr()

# Plot the matrix using a heatmap
plt.figure(figsize = (8, 6), dpi = 150)
# The vmin and vmax are set so the tickers aren't slightly off the bar 
# (looks nicer this way)
sns.heatmap(corr_matrix, annot = True, cmap = "coolwarm", vmin = -1.006, 
            vmax = 1.006, linecolor = "white", linewidth = 2)
plt.show()

# Wow, some of the banks are very heavily correlated
# This means that when the closing price for one bank is higher than the 
# previous day, it is generally true that the correlated banks will also be up
# on the day

"""# **Statistical Analysis**

## **ARIMA**

First, we will create one of the most popular types of Time Series models, ARIMA (Autoregressive Integrated Moving Average). For this model, we need to give the function three parameters - p (The number of lag observations included in the model, also called the lag order), d (The number of times that the raw observations are differenced, also called the degree of differencing) & q (The size of the moving average window, also called the order of moving average).
"""

# For creating the model
from statsmodels.tsa.arima_model import ARIMA
# For creating the grid to tune parameters
import itertools
# For rmse (root mean square error)
from math import sqrt
from sklearn.metrics import mean_squared_error
# First we want to find the optimal values of p, d & q
# We will do this using an AIC (Akaike information criterion) test
# It should be noted that alone, the value of AIC is basically useless
# It is only useful to compare multiple values, and the 'optimal' model is the 
# model with the lowest AIC 
# Create an array of values for each one and we will test them all to pick 
# the best one
p = d = q = range(0, 2)
# Note, we can make the spaces much smaller but the number of possible 
# Combinations increases VERY quickly
pdq = list(itertools.product(p, d, q))
pdq
seasonal_pdq = [(x[0], x[1], x[2], 12) for x in 
                list(itertools.product(p, d, q))]

# We will create a seperate model for each of the banks (we will also pick the
# optimal values for each bank)
# Use weekly resamples (again this is only to get rid of the Nan values which
# appear if we use daily since we don't have data for every day)
bac = bank_stocks["BAC"]["Close"].resample('W').mean()
aic = []
for i in pdq:
    for j in seasonal_pdq:
      model = sm.tsa.statespace.SARIMAX(bac, order = i, seasonal_order = j)
      results = model.fit()
      aic.append(results.aic)
      print('ARIMA{} x {} - AIC: {}'.format(i, j, results.aic))

# What is the lowest aic?
np.min(aic)
# This occurs with ARIMA(1, 1, 1 ) x (0, 0, 0, 12) so these are our optimal
# values for BAC
# We will do a similar process for every other bank too

# CitiGroup
c = bank_stocks["C"]["Close"].resample('W').mean()
aic = []
for i in pdq:
    for j in seasonal_pdq:
      model = sm.tsa.statespace.SARIMAX(c, order = i, seasonal_order = j)
      results = model.fit()
      aic.append(results.aic)
      print('ARIMA{} x {} - AIC: {}'.format(i, j, results.aic))

# What is the minimum aic and where does it occur?
np.min(aic)
# Occurs at ARIMA(1, 1, 0) x (1, 1, 1, 12)

# GS
gs = bank_stocks["GS"]["Close"].resample('W').mean()
aic = []
for i in pdq:
    for j in seasonal_pdq:
      model = sm.tsa.statespace.SARIMAX(gs, order = i, seasonal_order = j)
      results = model.fit()
      aic.append(results.aic)
      print('ARIMA{} x {} - AIC: {}'.format(i, j, results.aic))

np.min(aic)
# Occurs at ARIMA(0, 1, 1) x (0, 1, 1, 12)

# JPM
jpm = bank_stocks["JPM"]["Close"].resample('W').mean()
aic = []
for i in pdq:
    for j in seasonal_pdq:
      model = sm.tsa.statespace.SARIMAX(jpm, order = i, seasonal_order = j)
      results = model.fit()
      aic.append(results.aic)
      print('ARIMA{} x {} - AIC: {}'.format(i, j, results.aic))

np.min(aic)
# Occurs at ARIMA(0, 1, 1) x (0, 0, 0, 12)

# MS
ms = bank_stocks["MS"]["Close"].resample('W').mean()
aic = []
for i in pdq:
    for j in seasonal_pdq:
      model = sm.tsa.statespace.SARIMAX(ms, order = i, seasonal_order = j)
      results = model.fit()
      aic.append(results.aic)
      print('ARIMA{} x {} - AIC: {}'.format(i, j, results.aic))

np.min(aic)
# Occurs at ARIMA(0, 1, 1) x (0, 0, 1, 12)

# WFC
wfc = bank_stocks["WFC"]["Close"].resample('W').mean()
aic = []
for i in pdq:
    for j in seasonal_pdq:
      model = sm.tsa.statespace.SARIMAX(wfc, order = i, seasonal_order = j)
      results = model.fit()
      aic.append(results.aic)
      print('ARIMA{} x {} - AIC: {}'.format(i, j, results.aic))

np.min(aic)
# Occurs at ARIMA(0, 1, 1) x (1, 0, 1, 12)

# Ok, perfect, now that we are done the tuning portion, we will create the 
# optimal model for each of the banks and then we will try forecasting the 
# stock prices
# We will feed the model all of our data but we will try to make our model 
# predict some values for data that we already have and then we will also make
# it predict some values in the future 
# So the optimal values for the banks are the following 
# (In the format of (order, seasonal_order))
# BAC - (1, 1, 1), (0, 0, 0, 12)
# C - (1, 1, 0), (1, 1, 1, 12)
# GS - (0, 1, 1), (0, 1, 1, 12)
# JPM - (0, 1, 1), (0, 0, 0, 12)
# MS - (0, 1, 1), (0, 0, 1, 12)
# WFC - (0, 1, 1), (1, 0, 1, 12)

# What we want to see is how well does our model predict the stock prices
# So, this is more of a machine learning process rather than a statistical 
# approch. Therefore, rather than training the models on the whole data, like
# we used to find our parameters, we will train the models on our training set
# and we will test the results on the test set. For our training set, we will
# use all of the data up to, and including, 2017. Then, we will test our models
# on the 2018 & 2019 data
# Create the training sets
bac_training = bank_stocks["BAC"]["Close"].loc["2006-01-01":"2018-01-01"].\
               resample('W').mean()
c_training = bank_stocks["C"]["Close"].loc["2006-01-01":"2018-01-01"].\
             resample('W').mean()
gs_training = bank_stocks["GS"]["Close"].loc["2006-01-01":"2018-01-01"].\
              resample('W').mean()
jpm_training = bank_stocks["JPM"]["Close"].loc["2006-01-01":"2018-01-01"].\
               resample('W').mean()
ms_training = bank_stocks["MS"]["Close"].loc["2006-01-01":"2018-01-01"].\
              resample('W').mean()
wfc_training = bank_stocks["WFC"]["Close"].loc["2006-01-01":"2018-01-01"].\
               resample('W').mean()
# Let's create the models now 
bac_arima_model = sm.tsa.statespace.SARIMAX(bac_training, order = (1, 1, 1), 
                                            seasonal_order = (0, 0, 0, 12))
c_arima_model = sm.tsa.statespace.SARIMAX(c_training, order = (1, 1, 0), 
                                            seasonal_order = (1, 1, 1, 12))
gs_arima_model = sm.tsa.statespace.SARIMAX(gs_training, order = (0, 1, 1), 
                                            seasonal_order = (0, 1, 1, 12))
jpm_arima_model = sm.tsa.statespace.SARIMAX(jpm_training, order = (0, 1, 1), 
                                            seasonal_order = (0, 0, 0, 12))
ms_arima_model = sm.tsa.statespace.SARIMAX(ms_training, order = (0, 1, 1), 
                                            seasonal_order = (0, 0, 1, 12))
wfc_arima_model = sm.tsa.statespace.SARIMAX(wfc_training, order = (0, 1, 1), 
                                            seasonal_order = (1, 0, 1, 12))

# Ok, now that we created the models, let's see how they perform
# Rather than exploring the summary tables for all of them, we will pick one
# Bank and just explore that
# When we forecast the results, however, we will see how the model performs for 
# each bank
# Note, the AIC for this model will be different than before because we used
# the whole dataset before and now, to forecast, we are using the training set
print(bac_arima_model.fit(disp=0).summary())
# P-values are all very low (not actually 0 but rounds to 0 with the given 
# decimal values) which is a good sign
# We will check the rmse for each of the banks (after we forecast the results)
# and see which one if the highest (and lowest)
# Note: Goldman Sachs has the highest stock price so we would (kind of) expect
# that rmse to be the highest. For the rest of the stocks, the prices are 
# relatively similar so seeing which one is the highest/lowest for those banks
# will be much more interesting.

bac_arima_fit = bac_arima_model.fit(disp = False)
bac_pred = bac_arima_fit.get_forecast(steps = 100)
bac_ci = bac_pred.conf_int()

plt.figure(figsize = (12, 4), dpi = 300)
ax = bac.plot(label = 'observed', figsize=(12, 4), color = "black")
bac_pred.predicted_mean.plot(ax = ax, label = 'Forecast', color = "cyan")
ax.fill_between(bac_ci.index, bac_ci.iloc[:, 0], bac_ci.iloc[:, 1], color = 'k', 
                alpha = .25)
plt.legend()
plt.show()

# Ok so clearly something went wrong. Maybe we didn't tune our parameters 
# correctly. We only tried a handful of values because it takes a long time to
# run, but to try to make a better model, let's try to retune the parameters
# We will only try for BAC initially and see how that goes (hopefully it is 
# better than this)

# Ok so I tried running with p for 0, 1, 2, 3, 4
# d for 0, 1, 2
# q for 0, 1, 2
# And that takes a VERY long time, :(, so we will set some restrictions
# We won't test for p to be 4 so only p for 0, 1, 2, 3
# We also won't let d or q to be 0
# This is a reasonable assumption because when we did the tuning of parameters
# before, we barely got values of 0 (for the seasonal order there were
# a couple but we can see from the decomposition plots that seasonal doesn't
# play too large of a role)
p = range(0, 4)
d = range(1, 3)
q = range(1, 3)

pdq = list(itertools.product(p, d, q))
pdq
seasonal_pdq = [(x[0], x[1], x[2], 12) for x in 
                list(itertools.product(p, d, q))]

# We will create a seperate model for each of the banks (we will also pick the
# optimal values for each bank)
# Use weekly resamples (again this is only to get rid of the Nan values which
# appear if we use daily since we don't have data for every day)
bac = bank_stocks["BAC"]["Close"].resample('W').mean()
aic = []
for i in pdq:
    for j in seasonal_pdq:
      # Sometimes the program crashes so we want to bypass those errors
      try:
        model = sm.tsa.statespace.SARIMAX(bac, order = i, seasonal_order = j)
        results = model.fit()
        aic.append(results.aic)
        print('ARIMA{} x {} - AIC: {}'.format(i, j, results.aic))
      except: # Catch all exceptions 
        pass

# Ok so this is a VERY long process and we don't want to do this for every bank
# so for the sake of time, I will only find one set of p, d & q, and use that 
# for every bank. I know this is not the best way to do this, and if you are
# curious about any specific bank, feel free to run this code and change the
# name of the bank to create a better model for that specific stock.  
# For BAC, our lowest AIC value was 2049.4314110650216, so hopefully with this
# we find a set or parameters that will result in a lower AIC 

# NOTE: I DO NOT RECOMMEND RUNNING THIS CELL. IT TAKES ALMOST 3 HOURS
# So you can just view the results in the output below

# OK WE'RE DONE 
# Ok so now let's see what the lowest AIC value is and which set of 
# parameters get us that value
np.min(aic)
# Ok so this is higher than our other AIC 
# Use np.argmin(aic) to find the index
# ARIMA(0, 1, 2) x (0, 1, 1, 12)
# Why?
# Well because of our restrictions (d & p can't be 0), it probably 
# through something off. That's ok, let's see if this does a better job
# in predicting our results. For the other banks. We will compare this set
# of parameters to the ones we got before and pick the one with the lower
# AIC for the final model

# Let's plot this and see what happens
bac_arima_model2 = sm.tsa.statespace.SARIMAX(bac_training, order = (1, 1, 1), 
                                            seasonal_order = (0, 0, 0, 12))

bac_arima_fit2 = bac_arima_model2.fit(disp = False)
bac_pred = bac_arima_fit2.get_forecast(steps = 100)
bac_ci = bac_pred.conf_int()

plt.figure(figsize = (12, 4), dpi = 300)
ax = bac.plot(label = 'Close Price', figsize=(12, 4), color = "black")
bac_pred.predicted_mean.plot(ax = ax, label = 'Forecast', color = "tab:cyan")
bank_stocks["BAC"]["Close"].rolling(window = 30).mean().\
  plot(label = "30 Day Rolling Average", color = "tab:blue")
ax.fill_between(bac_ci.index, bac_ci.iloc[:, 0], bac_ci.iloc[:, 1], color = 'k', 
                alpha = .25)
plt.xlabel("Date")
plt.ylabel("Closing Price")
plt.title("Bank of America Closing Price and Forecasted Price")
plt.legend()
plt.show()
# Well, that was kind of anticlimactic :(
# So we will actually just leave the parameters the same for each bank and just
# show our results.
# Later, we will try to use the Prophet package from Facebook to fit these 
# results so hopefully that will give us better results

# CitiGroup
# We will actually just use the old results because that is probably a better
# model since it was tuned for these datasets specifically

c_arima_fit = c_arima_model.fit(disp = False)
c_pred = c_arima_fit.get_forecast(steps = 100)
c_ci = c_pred.conf_int()
c = bank_stocks["C"]["Close"].resample('W').mean()

plt.figure(figsize = (12, 4), dpi = 300)
ax = c.plot(label = 'Close Price', figsize=(12, 4), color = "black")
c_pred.predicted_mean.plot(ax = ax, label = 'Forecast', color = "tab:cyan")
bank_stocks["C"]["Close"].rolling(window = 30).mean().\
  plot(label = "30 Day Rolling Average", color = "tab:orange")
ax.fill_between(c_ci.index, c_ci.iloc[:, 0], c_ci.iloc[:, 1], color = 'k', 
                alpha = .25)
plt.xlabel("Date")
plt.ylabel("Closing Price")
plt.title("CitiGroup Closing Price and Forecasted Price")
plt.legend()
plt.show()

# Goldman Sachs
# We will actually just use the old results because that is probably a better
# model since it was tuned for these datasets specifically

gs_arima_fit = gs_arima_model.fit(disp = False)
gs_pred = gs_arima_fit.get_forecast(steps = 100)
gs_ci = gs_pred.conf_int()
gs = bank_stocks["GS"]["Close"].resample('W').mean()

plt.figure(figsize = (12, 4), dpi = 300)
ax = gs.plot(label = 'Close Price', figsize=(12, 4), color = "black")
gs_pred.predicted_mean.plot(ax = ax, label = 'Forecast', color = "tab:cyan")
bank_stocks["GS"]["Close"].rolling(window = 30).mean().\
  plot(label = "30 Day Rolling Average", color = "tab:green")
ax.fill_between(gs_ci.index, gs_ci.iloc[:, 0], gs_ci.iloc[:, 1], color = 'k', 
                alpha = .25)
plt.xlabel("Date")
plt.ylabel("Closing Price")
plt.title("Goldman Sachs Closing Price and Forecasted Price")
plt.legend()
plt.show()
# This one is not good, clearly it expects the trend to increase

# JPMorgan
jpm_arima_fit = jpm_arima_model.fit(disp = False)
jpm_pred = jpm_arima_fit.get_forecast(steps = 100)
jpm_ci = jpm_pred.conf_int()
jpm = bank_stocks["JPM"]["Close"].resample('W').mean()

plt.figure(figsize = (12, 4), dpi = 300)
ax = jpm.plot(label = 'Close Price', figsize=(12, 4), color = "black")
jpm_pred.predicted_mean.plot(ax = ax, label = 'Forecast', color = "tab:cyan")
bank_stocks["JPM"]["Close"].rolling(window = 30).mean().\
  plot(label = "30 Day Rolling Average", color = "tab:red")
ax.fill_between(jpm_ci.index, jpm_ci.iloc[:, 0], jpm_ci.iloc[:, 1], color = 'k', 
                alpha = .25)
plt.xlabel("Date")
plt.ylabel("Closing Price")
plt.title("JPMorgan Closing Price and Forecasted Price")
plt.legend()
plt.show()

# Morgan Stanley
ms_arima_fit = ms_arima_model.fit(disp = False)
ms_pred = ms_arima_fit.get_forecast(steps = 100)
ms_ci = ms_pred.conf_int()
ms = bank_stocks["MS"]["Close"].resample('W').mean()

plt.figure(figsize = (12, 4), dpi = 300)
ax = ms.plot(label = 'Close Price', figsize=(12, 4), color = "black")
ms_pred.predicted_mean.plot(ax = ax, label = 'Forecast', color = "tab:cyan")
bank_stocks["MS"]["Close"].rolling(window = 30).mean().\
  plot(label = "30 Day Rolling Average", color = "tab:purple")
ax.fill_between(ms_ci.index, ms_ci.iloc[:, 0], ms_ci.iloc[:, 1], color = 'k', 
                alpha = .25)
plt.xlabel("Date")
plt.ylabel("Closing Price")
plt.title("Morgan Stanley Closing Price and Forecasted Price")
plt.legend()
plt.show()

# Wells Fargo
wfc_arima_fit = wfc_arima_model.fit(disp = False)
wfc_pred = wfc_arima_fit.get_forecast(steps = 100)
wfc_ci = wfc_pred.conf_int()
wfc = bank_stocks["WFC"]["Close"].resample('W').mean()

plt.figure(figsize = (12, 4), dpi = 300)
ax = wfc.plot(label = 'Close Price', figsize=(12, 4), color = "black")
wfc_pred.predicted_mean.plot(ax = ax, label = 'Forecast', color = "tab:cyan")
bank_stocks["WFC"]["Close"].rolling(window = 30).mean().\
  plot(label = "30 Day Rolling Average", color = "tab:brown")
ax.fill_between(wfc_ci.index, wfc_ci.iloc[:, 0], wfc_ci.iloc[:, 1], color = 'k', 
                alpha = .25)
plt.xlabel("Date")
plt.ylabel("Closing Price")
plt.title("Wells Fargo Closing Price and Forecasted Price")
plt.legend()
plt.show()

"""Ok, so what happened? Well if we take a look at the decomposed plots, we see that a lot of the variability comes from the residual plot. If we only take a look at the trend and the seasonal plots, and then take a look at the forecasted prices, it isn't that bad. The problem is that we need the model to try and do a better job predicting the residual (which probably won't happen because we can't predict stock prices). BUT, we will try and do a better job than this in the future when we use the Prophet package which was developed by Facebook. """