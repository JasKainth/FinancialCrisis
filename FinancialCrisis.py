# -*- coding: utf-8 -*-
"""Finance.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UUSfCZJQm1T-ZaWh-fBji-1iNn21Y8G6

# **Financial Stocks Data**

For this dataset, we will perform an EDA on some stocks of banks from the financial crisis to early 2016. Later, we will also try to fit a regression model to try and capture the trend that the data follows.  
We will get the data for some of the biggest American Banks: 
* Bank of America
* CitiGroup
* Goldman Sachs
* JPMorgan Chase
* Morgan Stanley 
* Wells Fargo
"""

# Import some libraries
from pandas_datareader import data, wb # To get our data
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import datetime # For date format
import statsmodels.api as sm # For decomposing plots
sns.set_style("whitegrid") # Set a default seaborn style

# We will have the same start and end for all the banks
start = datetime.datetime(2006, 1, 1)
end = datetime.datetime(2020, 1, 1)

# Bank of America
BAC = data.DataReader("BAC", "yahoo", start, end)[["High", "Low", 
                                                   "Open", "Close", "Volume"]]
# For this we got the Date as an Index, High, Low, Open, Close, Volume and 
# Adjusted Close
# Grab everything other than the adjusted close
# CitiGroup
C = data.DataReader("C", "yahoo", start, end)[["High", "Low", 
                                                   "Open", "Close", "Volume"]]
# Goldman Sachs
GS = data.DataReader("GS", "yahoo", start, end)[["High", "Low", 
                                                   "Open", "Close", "Volume"]]
# JPMorgan Chase
JPM = data.DataReader("JPM", "yahoo", start, end)[["High", "Low", 
                                                   "Open", "Close", "Volume"]]
# Morgan Stanley
MS = data.DataReader("MS", "yahoo", start, end)[["High", "Low", 
                                                   "Open", "Close", "Volume"]]
# Wells Fargo
WFC = data.DataReader("WFC", "yahoo", start, end)[["High", "Low", 
                                                   "Open", "Close", "Volume"]]

# Create a list of tickers
tickers = "BAC C GS JPM MS WFC".split()
# Concatenate over columns (set axis = 1)
bank_stocks = pd.concat([BAC, C, GS, JPM, MS, WFC], axis = 1, keys = tickers)
# Add column names
bank_stocks.columns.names = ["Bank Ticker", "Stock Info"]
# What does our data look like right now?
bank_stocks.head()

"""# **EDA**

We will calculate the percent return for each stock. The return is defined as  
$r_t = \frac{p_t - p_{t-1}}{p_{t-1}} = \frac{p_t}{p_{t-1}} - 1$
"""

returns = pd.DataFrame()
for ticker in tickers:
  returns[ticker + " Return"] = bank_stocks[ticker]["Close"].pct_change()

# What does our returns df look like?
returns.head(10)
# Makes sense that the first one is NaN because we can't compare it to anything

# What trends do we see in the returns df?
sns.pairplot(returns[1:])
plt.show()
# Nothing seems too out of the ordinary, all of them are heavily centered 
# around 0

# What does the distribution of the returns look like over the years?
# Make another dataset in a different format;
# We will have 4 columns (3 and we create one later called Year)
# Make one column for the date, one for the bank and one for the return of the
# bank at that date
# Reset the index (easier to make plots with one index rather than multileve
# index)
long_banks = pd.DataFrame(returns.stack()).reset_index()
long_banks.columns = ["Date", "Bank", "Return"]
# Use a lammbda function to get the bank name (the entries right now are 
# "Bank_Name Return" so remove the "Return")
long_banks["Bank"] = long_banks["Bank"].apply(lambda x: x.split()[0])
# Get the year (for facet plot)
long_banks["Year"] = pd.DatetimeIndex(long_banks["Date"]).year
# Create a plot of the returns for each bank over the 14 years of data we have
g = sns.FacetGrid(long_banks, col = "Bank",  row = "Year", hue = "Bank")
g.map(sns.histplot, "Return")
plt.show()
# We see that in 2006 & 2007, there is very low variability
# However, in 2008 & 2009, there is a lot of variability and the peaks are not 
# as concentrated as the other years 
# After that, 2010 & 2011 is better than '08 & '09 but in 2012 & 2013 do we 
# finally see the returns get back to "normal"

# Lets compare data from 2 years (2008 and 2019) for each bank, side by side
long_banks_filtered = long_banks[(long_banks["Year"] == 2008) |
                                 (long_banks["Year"] == 2019)]
g = sns.FacetGrid(long_banks_filtered, col = "Year",  
                  row = "Bank", hue = "Bank")
g.map(sns.histplot, "Return")
plt.show()

# When do the highest and lowest returns occur for each bank?
returns.idxmin()
# WOW, so they are all heavily around the financial crisis (which makes sense)
# But one thing which was unexpected is that for FOUR out of the SIX banks, the
# Minimum occurs on the same day (BAC, GS, JPM & WFC) on 2009-01-20
# On January 20, 2009 is when Barack Obama for inaugurated.

# What about the max?
returns.idxmax()
# Funny thing about MS, 4 days after their lowest return they had their highest
# and for JPM the highest was the very next day

# During the financial crisis, there was a new president so there was a lot of
# changes occuring, which is why there is probably a lot of variability during
# that period. We will probably see this for all the banks when we plot the 
# trend lines for each of the banks (let's see how this changes the 7 day and
# 30 day rolling average, which we will also plot)

# What does the overall SD of each stock look like 
# Look at SD for the whole dataset, and also for 2007 - 2010
returns.std()
# BAC, C & MS have a relatively high std compared to GS, JPM & WFC

# Let's take a look at the std over 4 years
a = returns.loc["2007-01-01":"2007-12-31"].std()
b = returns.loc["2008-01-01":"2008-12-31"].std()
c = returns.loc["2009-01-01":"2009-12-31"].std()
d = returns.loc["2010-01-01":"2010-12-31"].std()

sd = pd.concat([a, b, c, d], axis = 1, keys = [2007, 2008, 2009, 2010])
# As we can see, the std is pretty low in 2007, but increases a lot in 
# 2008 & 2009 (which we expect) and begins to settle down again in 2010
sd

# What does the std look like in the most recent full year (2019)
returns.loc["2019-01-01":"2019-12-31"].std()
# They are all very low

# Let's take a look at the closing price for each stock
# How does it vary over time
# CitiGroup crashed quite a bit in 2008 and didn't really recover

# Resize the figure (and change the dpi to make it bigger/nicer)
plt.figure(figsize = (12, 4), dpi = 300)
# Run a for loop that will go over each ticker symbol (bank) and plot the Close
# price for each bank
for ticker in tickers:
  # Plot the data and add a label for the ticker
  bank_stocks[ticker]["Close"].plot(label = ticker)
plt.legend()
plt.xlabel("Date")
plt.ylabel("Closing Price")
plt.title("Closing Price of Bank Stocks")
plt.show()

# Let's see a breakdown of each Bank's trend
# BAC
# We will make the period a week (this will help us get rid of some NAs) but 
# will also make the format of the DataFrame a bit nicer
bac = bank_stocks["BAC"]["Close"].resample('W').mean()
# Decompose the trend (into an additive model)
decomposition = sm.tsa.seasonal_decompose(bac, model = "additive")
# Create the figure and axes (using the object oriented method)
# Note: We could have simply called the method .plot() on our decomposition 
# dataframe but that would not allow us to make any changes 
# So we would not be able to to change the figure size, dpi or color of the 
# plots
fig, axes = plt.subplots(4, 1, sharex=True, figsize = (8, 4), dpi = 300)
decomposition.observed.plot(ax=axes[0], legend=False)
axes[0].set_ylabel("Observed")
axes[0].set_title("Bank of America Close Price Decomposition")
decomposition.trend.plot(ax=axes[1], legend=False)
axes[1].set_ylabel("Trend")
decomposition.seasonal.plot(ax=axes[2], legend=False)
axes[2].set_ylabel("Seasonal")
decomposition.resid.plot(ax=axes[3], legend=False)
axes[3].set_ylabel("Residual")
plt.show()

# C
c = bank_stocks["C"]["Close"].resample('W').mean()
decomposition = sm.tsa.seasonal_decompose(c, model = "additive")
fig, axes = plt.subplots(4, 1, sharex=True, figsize = (8, 4), dpi = 300)
decomposition.observed.plot(ax=axes[0], legend=False, color = "orange")
axes[0].set_ylabel("Observed")
axes[0].set_title("CitiGroup Close Price Decomposition")
decomposition.trend.plot(ax=axes[1], legend=False, color = "orange")
axes[1].set_ylabel("Trend")
decomposition.seasonal.plot(ax=axes[2], legend=False, color = "orange")
axes[2].set_ylabel("Seasonal")
decomposition.resid.plot(ax=axes[3], legend=False, color = "orange")
axes[3].set_ylabel("Residual")
plt.show()

# GS
gs = bank_stocks["GS"]["Close"].resample('W').mean()
decomposition = sm.tsa.seasonal_decompose(gs, model = "additive")
fig, axes = plt.subplots(4, 1, sharex=True, figsize = (8, 4), dpi = 300)
decomposition.observed.plot(ax=axes[0], legend=False, color = "green")
axes[0].set_ylabel("Observed")
axes[0].set_title("Goldman Sachs Close Price Decomposition")
decomposition.trend.plot(ax=axes[1], legend=False, color = "green")
axes[1].set_ylabel("Trend")
decomposition.seasonal.plot(ax=axes[2], legend=False, color = "green")
axes[2].set_ylabel("Seasonal")
decomposition.resid.plot(ax=axes[3], legend=False, color = "green")
axes[3].set_ylabel("Residual")
plt.show()

# JPM
jpm = bank_stocks["JPM"]["Close"].resample('W').mean()
decomposition = sm.tsa.seasonal_decompose(jpm, model = "additive")
fig, axes = plt.subplots(4, 1, sharex=True, figsize = (8, 4), dpi = 300)
decomposition.observed.plot(ax=axes[0], legend=False, color = "red")
axes[0].set_ylabel("Observed")
axes[0].set_title("JPMorgan Close Price Decomposition")
decomposition.trend.plot(ax=axes[1], legend=False, color = "red")
axes[1].set_ylabel("Trend")
decomposition.seasonal.plot(ax=axes[2], legend=False, color = "red")
axes[2].set_ylabel("Seasonal")
decomposition.resid.plot(ax=axes[3], legend=False, color = "red")
axes[3].set_ylabel("Residual")
plt.show()

# MS
ms = bank_stocks["MS"]["Close"].resample('W').mean()
decomposition = sm.tsa.seasonal_decompose(ms, model = "additive")
fig, axes = plt.subplots(4, 1, sharex=True, figsize = (8, 4), dpi = 300)
decomposition.observed.plot(ax=axes[0], legend=False, color = "purple")
axes[0].set_ylabel("Observed")
axes[0].set_title("Morgan Stanley Close Price Decomposition")
decomposition.trend.plot(ax=axes[1], legend=False, color = "purple")
axes[1].set_ylabel("Trend")
decomposition.seasonal.plot(ax=axes[2], legend=False, color = "purple")
axes[2].set_ylabel("Seasonal")
decomposition.resid.plot(ax=axes[3], legend=False, color = "purple")
axes[3].set_ylabel("Residual")
plt.show()

# WFC
wfc = bank_stocks["WFC"]["Close"].resample('W').mean()
decomposition = sm.tsa.seasonal_decompose(wfc, model = "additive")
fig, axes = plt.subplots(4, 1, sharex=True, figsize = (8, 4), dpi = 300)
decomposition.observed.plot(ax=axes[0], legend=False, color = "brown")
axes[0].set_ylabel("Observed")
axes[0].set_title("Wells Fargo Close Price Decomposition")
decomposition.trend.plot(ax=axes[1], legend=False, color = "brown")
axes[1].set_ylabel("Trend")
decomposition.seasonal.plot(ax=axes[2], legend=False, color = "brown")
axes[2].set_ylabel("Seasonal")
decomposition.resid.plot(ax=axes[3], legend=False, color = "brown")
axes[3].set_ylabel("Residual")
plt.show()

"""**What do we see from those plots?**  
The first time that is noticeable from the trend is all of them take a dip in 2008-2009. Some of them, like WFC, recover but some of them, like C, don't. We also see that some banks have multiple dips. Most of them aren't too bad but in the case of GS, it is quite a large dip. It should be noted that the seasonal trend is negligible since when we take a look at the y-axes, the values are very small will often get overshadowed by the Residual.
"""

# For most of these banks, there was a lot of variability in 2008 & 2009
# So, let's take a look at the 30 day and 7 day rolling average during that 
# time period
# Will also add 2010 just so we see how the begin to recover
# BAC 
# Create the figure and specify the size and dpi
plt.figure(figsize = (12, 4), dpi = 300)
# Plot the Closing prices
bank_stocks["BAC"]["Close"].loc["2008-01-01":"2011-01-01"].\
  plot(label = "BAC Close Price")
# We use the rolling() function to create a 7 & 30 day rolling average
bank_stocks["BAC"]["Close"].loc["2008-01-01":"2011-01-01"].\
  rolling(window = 7).mean().plot(label = "7 Day Rolling Average")
bank_stocks["BAC"]["Close"].loc["2008-01-01":"2011-01-01"].\
  rolling(window = 30).mean().plot(label = "30 Day Rolling Average")
plt.legend()
plt.xlabel("Date")
plt.ylabel("Closing Price")
plt.title("Bank of America Closing Price")
plt.show()

# C
plt.figure(figsize = (12, 4), dpi = 300)
bank_stocks["C"]["Close"].loc["2008-01-01":"2011-01-01"].\
  plot(label = "C Close Price")
bank_stocks["C"]["Close"].loc["2008-01-01":"2011-01-01"].\
  rolling(window = 7).mean().plot(label = "7 Day Rolling Average")
bank_stocks["C"]["Close"].loc["2008-01-01":"2011-01-01"].\
  rolling(window = 30).mean().plot(label = "30 Day Rolling Average")
plt.legend()
plt.xlabel("Date")
plt.ylabel("Closing Price")
plt.title("CitiGroup Closing Price")
plt.show()

# GS
plt.figure(figsize = (12, 4), dpi = 300)
bank_stocks["GS"]["Close"].loc["2008-01-01":"2011-01-01"].\
  plot(label = "GS Close Price")
bank_stocks["GS"]["Close"].loc["2008-01-01":"2011-01-01"].\
  rolling(window = 7).mean().plot(label = "7 Day Rolling Average")
bank_stocks["GS"]["Close"].loc["2008-01-01":"2011-01-01"].\
  rolling(window = 30).mean().plot(label = "30 Day Rolling Average")
plt.legend()
plt.xlabel("Date")
plt.ylabel("Closing Price")
plt.title("Goldman Sachs Closing Price")
plt.show()

# JPM
plt.figure(figsize = (12, 4), dpi = 300)
bank_stocks["JPM"]["Close"].loc["2008-01-01":"2011-01-01"].\
  plot(label = "JPM Close Price")
bank_stocks["JPM"]["Close"].loc["2008-01-01":"2011-01-01"].\
  rolling(window = 7).mean().plot(label = "7 Day Rolling Average")
bank_stocks["JPM"]["Close"].loc["2008-01-01":"2011-01-01"].\
  rolling(window = 30).mean().plot(label = "30 Day Rolling Average")
plt.legend()
plt.xlabel("Date")
plt.ylabel("Closing Price")
plt.title("JPMorgan Closing Price")
plt.show()

# MS
plt.figure(figsize = (12, 4), dpi = 300)
bank_stocks["MS"]["Close"].loc["2008-01-01":"2011-01-01"].\
  plot(label = "MS Close Price")
bank_stocks["MS"]["Close"].loc["2008-01-01":"2011-01-01"].\
  rolling(window = 7).mean().plot(label = "7 Day Rolling Average")
bank_stocks["MS"]["Close"].loc["2008-01-01":"2011-01-01"].\
  rolling(window = 30).mean().plot(label = "30 Day Rolling Average")
plt.legend()
plt.xlabel("Date")
plt.ylabel("Closing Price")
plt.title("Morgan Stanley Closing Price")
plt.show()

# WFC
plt.figure(figsize = (12, 4), dpi = 300)
bank_stocks["WFC"]["Close"].loc["2008-01-01":"2011-01-01"].\
  plot(label = "WFC Close Price")
bank_stocks["WFC"]["Close"].loc["2008-01-01":"2011-01-01"].\
  rolling(window = 7).mean().plot(label = "7 Day Rolling Average")
bank_stocks["WFC"]["Close"].loc["2008-01-01":"2011-01-01"].\
  rolling(window = 30).mean().plot(label = "30 Day Rolling Average")
plt.legend()
plt.xlabel("Date")
plt.ylabel("Closing Price")
plt.title("Wells Fargo Closing Price")
plt.show()

# How are the stock prices correlated among each bank? (Look at closing price)
# Create a correlation matrix
corr_matrix = bank_stocks.xs(key = "Close", axis = 1, level = "Stock Info").\
              corr()

# Plot the matrix using a heatmap
plt.figure(figsize = (8, 6), dpi = 150)
# The vmin and vmax are set so the tickers aren't slightly off the bar 
# (looks nicer this way)
sns.heatmap(corr_matrix, annot = True, cmap = "coolwarm", vmin = -1.006, 
            vmax = 1.006, linecolor = "white", linewidth = 2)
plt.show()

# Wow, some of the banks are very heavily correlated
# This means that when the closing price for one bank is higher than the 
# previous day, it is generally true that the correlated banks will also be up
# on the day